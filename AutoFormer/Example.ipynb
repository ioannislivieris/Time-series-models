{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Basic libraries\n",
    "#\n",
    "import random\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from   pickle import dump\n",
    "from   tqdm   import tqdm\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Sklearn library\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Visualization library\n",
    "#\n",
    "import matplotlib.pyplot   as plt \n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# PyTorch library\n",
    "#\n",
    "import torch\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# User library\n",
    "#\n",
    "from data_loader.data_loader  import *\n",
    "from utils.PerformanceMetrics import RegressionEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the seed \n",
    "#\n",
    "seed = 42\n",
    "random.seed( seed )\n",
    "torch.manual_seed( seed )\n",
    "np.random.seed( seed )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        self.description = 'Autoformer & Transformer family for Time Series Forecasting'\n",
    "\n",
    "        \n",
    "        # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "        # Basic config\n",
    "        #\n",
    "        #\n",
    "        # Model ID\n",
    "        self.model_id = 'Exchange_rate_24_24'\n",
    "        # Select model, options: [Autoformer, Informer, Transformer]\n",
    "        self.model    = 'Autoformer'\n",
    "\n",
    "\n",
    "        # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "        # Data loader\n",
    "        #\n",
    "        #\n",
    "        # Root path of the data file\n",
    "        self.root_path = './Datasets/exchange_rate/'\n",
    "        # Data filename\n",
    "        self.data_path = 'exchange_rate.csv'\n",
    "        # Target feature in S or MS task\n",
    "        self.target = 'OT'\n",
    "        # Log-transformation\n",
    "        self.transformation = True\n",
    "        # Scaling, options: ['Standard', 'Robust', MinMax']\n",
    "        self.scaling = 'Standard'\n",
    "        # Forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, \n",
    "        # S:univariate predict univariate, MS: multivariate predict univariate\n",
    "        self.features  = 'M'\n",
    "        # Freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, \n",
    "        # b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "        self.freq      = 'h'              \n",
    "        # Location of model checkpoints\n",
    "        self.checkpoints = './checkpoints/'\n",
    "        # Save examples during testing\n",
    "        self.saveExamples = False\n",
    "        \n",
    "        # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "        # Forecasting task\n",
    "        #\n",
    "        #\n",
    "        # Input sequence length\n",
    "        self.seq_len   = 24  \n",
    "        # Start token length\n",
    "        self.label_len = 12  \n",
    "        # Prediction sequence length\n",
    "        self.pred_len  = 24  \n",
    "\n",
    "\n",
    "\n",
    "        # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "        # Model hyper-parameters\n",
    "        #\n",
    "        #\n",
    "        # Encoder input size\n",
    "        self.enc_in     = 8\n",
    "        # Decoder input size        \n",
    "        self.dec_in     = 8\n",
    "        # Output size\n",
    "        self.c_out      = 8\n",
    "        # Dimension of the model\n",
    "        self.d_model    = 512\n",
    "        # Number of heads\n",
    "        self.n_heads    = 8\n",
    "        # Number of encoder layers\n",
    "        self.e_layers   = 2\n",
    "        # Number of decoder layers\n",
    "        self.d_layers   = 1\n",
    "        # Dimension of fcn\n",
    "        self.d_ff       = 2048\n",
    "        # Window size of moving average\n",
    "        self.moving_avg = 25\n",
    "        # Attention factor\n",
    "        self.factor     = 3\n",
    "        # Whether to use distilling in encoder, using this argument means not using distilling\n",
    "        self.distil     = True\n",
    "        # Dropout rate\n",
    "        self.dropout    = 0.05\n",
    "        # Time features encoding, options:[timeF, fixed, learned]\n",
    "        self.embed      = 'timeF'\n",
    "        # Activation function\n",
    "        self.activation       = 'gelu'\n",
    "        # Whether to output attention in encoder\n",
    "        self.output_attention = False\n",
    "\n",
    "\n",
    "\n",
    "        # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "        # Optimization\n",
    "        #\n",
    "        #\n",
    "        # Number of workers in data-loader\n",
    "        self.num_workers   = 0 # NOTE: if num_workers > 0 CUDA stops working after a while\n",
    "        # Training epochs\n",
    "        self.train_epochs  = 100\n",
    "        # Batch size\n",
    "        self.batch_size    = 32\n",
    "        # Early stopping patience\n",
    "        self.patience      = 10\n",
    "        # Optimizer, options ['Adam', 'SGD', 'Adagrad']\n",
    "        self.optimizer     = 'Adagrad'\n",
    "        # Optimization learning rate\n",
    "        self.learning_rate = 0.0001\n",
    "        # Momentum (in case Optimizer = 'SGD')\n",
    "        self.momentum = 0.9\n",
    "        # Experiment description\n",
    "        self.des           = 'Exp'\n",
    "        # Loss function, options: [MSE, MAE, MAPE, SMAPE]\n",
    "        self.loss          = 'MSE' \n",
    "        # adjust learning rate, options: ['type1', 'type2', 'Scheduler']\n",
    "        self.lradj         = 'Scheduler' \n",
    "        # use automatic mixed precision training\n",
    "        self.use_amp       = False\n",
    "\n",
    "        \n",
    "        # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "        # GPU\n",
    "        #\n",
    "        #\n",
    "        # Use gpu\n",
    "        self.use_gpu       = True\n",
    "        # Selected GPU\n",
    "        self.gpu           = 0\n",
    "        # use multi-gpu\n",
    "        self.use_multi_gpu = False\n",
    "        # device ids of multile gpus\n",
    "        self.devices       = '0,1,2,3'   \n",
    "        \n",
    "    def print(self):\n",
    "        d = self.__dict__\n",
    "        for x in d.keys():\n",
    "            print('{}: {}'.format(x, d[x]))\n",
    "    \n",
    "    def save(self, path = '.'):\n",
    "        import json\n",
    "        d = self.__dict__\n",
    "        \n",
    "        # create json object from dictionary\n",
    "        json = json.dumps( d )\n",
    "        \n",
    "        \n",
    "        # open file for writing, \"w\" \n",
    "        f = open(os.path.join(path, \"Parameters.json\"), \"w\")\n",
    "\n",
    "        # write json object to file\n",
    "        f.write(json)\n",
    "\n",
    "        # close file\n",
    "        f.close()\n",
    "        \n",
    "        print('[INFO] Parameters saved in file: ', os.path.join(path, \"Parameters.json\"))\n",
    "        \n",
    "args = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting record of experiments\n",
    "#\n",
    "setting = '{}_{}'.format( args.model_id, args.model )\n",
    "# setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}'.format(\n",
    "#     args.model_id,\n",
    "#     args.model,\n",
    "#     args.features,\n",
    "#     args.seq_len,\n",
    "#     args.label_len,\n",
    "#     args.pred_len,\n",
    "#     args.d_model,\n",
    "#     args.n_heads,\n",
    "#     args.e_layers,\n",
    "#     args.d_layers,\n",
    "#     args.d_ff,\n",
    "#     args.factor,\n",
    "#     args.embed,\n",
    "#     args.distil,\n",
    "#     args.des, \n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# df_raw.columns: ['date', ...(other features), target feature]\n",
    "# '''\n",
    "#\n",
    "df = pd.read_csv( args.root_path + args.data_path )\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Date as index\n",
    "#\n",
    "df['date'] = df['date'].astype('datetime64')\n",
    "df.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training/testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index for splitting training/testing sets \n",
    "# - 80% of data are utilized for training \n",
    "# - 20% of data are utilized for testing\n",
    "train_idx = int(len(df) * 0.8)\n",
    "\n",
    "\n",
    "# Create training set\n",
    "#\n",
    "df_train = df[:train_idx]\n",
    "df_test  = df[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index for splitting training/validations sets \n",
    "# - 10% of training data are utilized for validation\n",
    "valid_idx = int(len(df_train) * 0.9)\n",
    "\n",
    "\n",
    "# Create training set\n",
    "#\n",
    "df_valid = df_train[valid_idx:]\n",
    "df_train = df_train[:valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[args.target] .plot( figsize=(20, 3) );\n",
    "df_valid[args.target] .plot( );\n",
    "df_test[args.target] .plot( );\n",
    "#\n",
    "plt.legend(['Training', 'Validation', 'Testing'], fontsize = 14, frameon = False);\n",
    "plt.xlabel('Time', size = 14);\n",
    "plt.xticks(size = 12);\n",
    "plt.ylabel(args.target, size = 14);\n",
    "plt.yticks(size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.concat([df_train.iloc[-args.seq_len:], df_valid])\n",
    "df_test  = pd.concat([df_valid.iloc[-args.seq_len:], df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( args.transformation ):\n",
    "    df_train = np.log( df_train )\n",
    "    df_valid = np.log( df_valid )\n",
    "    df_test  = np.log( df_test  )\n",
    "    \n",
    "    print('[INFO] Data were transformed (Log)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if   (args.scaling == 'Standard'):\n",
    "    print('[INFO] Standard scaler')\n",
    "    scaler = StandardScaler()\n",
    "#\n",
    "elif (args.scaling == 'Robust'):\n",
    "    print('[INFO] Robust scaler')\n",
    "    scaler = RobustScaler()\n",
    "#\n",
    "elif (args.scaling == 'MinMax'):\n",
    "    print('[INFO] MinMax scaler')    \n",
    "    scaler = MinMaxScaler()\n",
    "else:\n",
    "    print('[ERROR] Invalid selection of scaler')\n",
    "    print('[INFO] Standard scaler is selected')\n",
    "    args.scaling = 'Standard'\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Scaling training data\n",
    "#\n",
    "df_train = pd.DataFrame(data    = scaler.fit_transform( df_train ),\n",
    "                        index   = df_train.index,\n",
    "                        columns = df_train.columns)\n",
    "\n",
    "\n",
    "# Scaling testing data\n",
    "#\n",
    "df_valid = pd.DataFrame(data     = scaler.transform( df_valid ),\n",
    "                        index    = df_valid.index,\n",
    "                        columns  = df_valid.columns)\n",
    "\n",
    "# Scaling testing data\n",
    "#\n",
    "df_test = pd.DataFrame(data     = scaler.transform( df_test ),\n",
    "                        index   = df_test.index,\n",
    "                        columns = df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for model checkpoints\n",
    "#\n",
    "path = os.path.join(args.checkpoints, setting)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "               \n",
    "dump(scaler, open(path + '/Scaler.pkl', 'wb'))\n",
    "print('[INFO] Scaler saved in file: ', path + '/Scaler.pkl')\n",
    "\n",
    "# Saving parameters\n",
    "#\n",
    "args.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeenc      = 0 if args.embed != 'timeF' else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_flag = True\n",
    "drop_last    = True\n",
    "\n",
    "train_data = createDataset(df        = df_train,\n",
    "                           size      = [args.seq_len, args.label_len, args.pred_len],\n",
    "                           features  = args.features,\n",
    "                           timeenc   = timeenc,\n",
    "                           freq      = args.freq,\n",
    "                        )\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size  = args.batch_size,\n",
    "                          shuffle     = shuffle_flag,\n",
    "                          num_workers = args.num_workers,\n",
    "                          drop_last   = drop_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_flag = True\n",
    "drop_last    = True\n",
    "\n",
    "valid_data = createDataset(df        = df_valid,\n",
    "                           size      = [args.seq_len, args.label_len, args.pred_len],\n",
    "                           features  = args.features,\n",
    "                           timeenc   = timeenc,\n",
    "                           freq      = args.freq,\n",
    "                        )\n",
    "\n",
    "valid_loader = DataLoader(valid_data,\n",
    "                          batch_size  = args.batch_size,\n",
    "                          shuffle     = shuffle_flag,\n",
    "                          num_workers = args.num_workers,\n",
    "                          drop_last   = drop_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_flag = False\n",
    "drop_last    = False\n",
    "\n",
    "test_data = createDataset(df          = df_test,\n",
    "                          size        = [args.seq_len, args.label_len, args.pred_len],\n",
    "                          features    = args.features,\n",
    "                          timeenc     = timeenc,\n",
    "                          freq        = args.freq)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                          batch_size  = args.batch_size,\n",
    "                          shuffle     = shuffle_flag,\n",
    "                          num_workers = args.num_workers,\n",
    "                          drop_last   = drop_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.exp_main import Exp_Main\n",
    "\n",
    "# Set experiment\n",
    "#\n",
    "Exp = Exp_Main\n",
    "exp = Exp( args )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = exp.train(setting, train_loader, valid_loader, test_loader);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device for inference\n",
    "#\n",
    "device = exp.device\n",
    " \n",
    "    \n",
    "testY, y_pred = None, None\n",
    "#\n",
    "#\n",
    "with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "    for Iter, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tepoch):\n",
    "        tepoch.set_description(f\"Iteration: {Iter + 1}/{len(test_loader)}\")\n",
    "    \n",
    "        batch_x = batch_x.float().to( device )\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to( device )\n",
    "        batch_y_mark = batch_y_mark.float().to( device )\n",
    "\n",
    "        # Decoder input\n",
    "        #\n",
    "        dec_inp = torch.zeros_like(batch_x).float()\n",
    "        dec_inp = torch.cat([batch_x[:, -args.label_len:, :], dec_inp], dim=1).float().to( device )\n",
    "\n",
    "        # Encoder - Decoder\n",
    "        #\n",
    "        if args.use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                if args.output_attention:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                else:\n",
    "                    outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        else:\n",
    "            if args.output_attention:\n",
    "                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "\n",
    "            else:\n",
    "                outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "\n",
    "\n",
    "        # Get predictions and real values\n",
    "        #\n",
    "        pred    = outputs[:, -args.pred_len:, f_dim:]\n",
    "        true    = batch_y[:, -args.pred_len:, f_dim:].to( device )\n",
    "\n",
    "        # Convert to NumPy arrays\n",
    "        #\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        true = true.detach().cpu().numpy()\n",
    "\n",
    "        # Inverse trasformation\n",
    "        #\n",
    "        pred = scaler.inverse_transform( pred )\n",
    "        true = scaler.inverse_transform( true )\n",
    "        if ( args.transformation ):\n",
    "            pred = np.exp( pred )\n",
    "            true = np.exp( true )\n",
    "\n",
    "\n",
    "        # Store predictions & Real values\n",
    "        #\n",
    "        if (testY is None):\n",
    "            y_pred = pred\n",
    "            testY  = true\n",
    "        else:\n",
    "            y_pred = np.concatenate([y_pred, pred], axis = 0)\n",
    "            testY  = np.concatenate([testY,  true], axis = 0)  \n",
    "\n",
    "            \n",
    "         \n",
    "        \n",
    "        # Print Examples to pdf\n",
    "        #\n",
    "        if (args.saveExamples and Iter % 10 == 0):\n",
    "            from utils.tools import visual\n",
    "            \n",
    "            # Get input\n",
    "            #\n",
    "            inputs = batch_x.detach().cpu().numpy()\n",
    "\n",
    "            # Inverse trasformation\n",
    "            #\n",
    "            inputs = scaler.inverse_transform( inputs )\n",
    "            if ( args.transformation ):\n",
    "                inputs = np.exp( inputs )\n",
    "\n",
    "            # Select only target values\n",
    "            #\n",
    "            inputs       = inputs[:, :, -1]\n",
    "            GroundTruth = true[:, :, -1]\n",
    "            Predictions = pred[:, :, -1]\n",
    "\n",
    "\n",
    "            # Create directory for model checkpoints\n",
    "            #\n",
    "            Images_path = os.path.join(args.checkpoints, setting, 'pics')\n",
    "            if not os.path.exists(Images_path):\n",
    "                os.makedirs(Images_path)\n",
    "\n",
    "            visual(input, GroundTruth, Predictions, figsize = (20, 3), name = os.path.join(Images_path, f'{Iter}.pdf'));    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY  = testY[:,:,-1]\n",
    "y_pred = y_pred[:,:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Performance on Testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance_Foresting_Model = {'RMSE': [], 'MAE': [], 'SMAPE': [], 'R2' : []}\n",
    "\n",
    "filename = os.path.join(args.checkpoints, setting, 'results.txt')\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    # Legend\n",
    "    f.write('Horizon\\t   MAE   SMAPE    RMSE     R2\\n')\n",
    "    for i in range( args.pred_len ):\n",
    "\n",
    "        # Create DataFrame\n",
    "        #\n",
    "        Prices = pd.DataFrame([])        \n",
    "        \n",
    "        # Include Real values and Predictions\n",
    "        #\n",
    "        Prices['Real']       = testY[:,  i]\n",
    "        Prices['Prediction'] = y_pred[:, i] \n",
    "\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        #\n",
    "        MAE, RMSE, MAPE, SMAPE, R2 = RegressionEvaluation( Prices )\n",
    "\n",
    "        # Print results\n",
    "        #\n",
    "        print('Horizon: %2i  MAE: %.2f  RMSE: %.2f  SMAPE: %.2f  R2: %.2f' % (i+1, MAE, RMSE, SMAPE, R2) )\n",
    "        f.write('{:7.0f}\\t {:.3f}   {:.3f}   {:.3f}  {:.3f}\\n'.format(i+1, MAE, SMAPE, RMSE, R2))\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Performance_Foresting_Model = {'RMSE': [], 'MAE': [], 'SMAPE': [], 'R2' : []}\n",
    "\n",
    "for i in range( 6 ):\n",
    "\n",
    "    Prices = pd.DataFrame([])        \n",
    "\n",
    "    Prices['Real']       = testY[:,  i]\n",
    "    Prices['Prediction'] = y_pred[:, i] \n",
    "            \n",
    "    \n",
    "    # Plot Real & Predicted values\n",
    "    #\n",
    "    Prices[:100].plot( figsize = (20, 3), marker = 'o' )\n",
    "    #\n",
    "    plt.title('Feature: {} - Horizon = {}'.format('Target', i+1))\n",
    "    plt.legend( frameon = False, fontsize = 14)\n",
    "    plt.xticks(size = 12)\n",
    "    plt.yticks(size = 12)\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots = [331, 332, 333, 334, 335, 336,  337, 338, 339]\n",
    "plt.figure( figsize = (30, 10) )\n",
    "RandomInstances = [random.randint(1,testY.shape[0]) for i in range(0, 9)]\n",
    "\n",
    "for plot_id, i in enumerate(RandomInstances):\n",
    "    \n",
    "    plt.subplot(subplots[plot_id])\n",
    "    plt.grid()\n",
    "    plt.plot(range(args.pred_len), testY[i],  color='g', marker = 'o')\n",
    "    plt.plot(range(args.pred_len), y_pred[i], color='r', marker = 'o')\n",
    "\n",
    "    plt.legend(['Future values', 'Prediction'], frameon = False, fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dev_ili_pytorch]",
   "language": "python",
   "name": "conda-env-.conda-dev_ili_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
