{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Remove warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Basic libraries\n",
    "#\n",
    "import time\n",
    "import random\n",
    "import pandas    as pd\n",
    "import numpy     as np\n",
    "from   tqdm      import tqdm\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Visualization library\n",
    "#\n",
    "import matplotlib.pyplot   as plt \n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Sklearn library\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "# Torch libraries\n",
    "#\n",
    "import torch\n",
    "import torch.nn                     as nn\n",
    "import torch.nn.functional          as F\n",
    "from   torch.utils.data             import DataLoader\n",
    "from   torch.utils.data             import Dataset\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "# User libraries\n",
    "#\n",
    "from utils.PerformanceMetrics import RegressionEvaluation\n",
    "from utils.EarlyStopping      import *\n",
    "from utils.LRScheduler        import *\n",
    "\n",
    "\n",
    "\n",
    "from models.DeepTIMe import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gpus = tensorflow.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tensorflow.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "            device = torch.device( 'cuda:0' ) \n",
    "            \n",
    "            logical_gpus = tensorflow.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            \n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "            \n",
    "            device = torch.device( 'cpu' )\n",
    "except:\n",
    "    device = torch.device( 'cpu' )\n",
    "    print('[INFO] Not GPU found - CPU selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        self.description = 'DLinear model for time-series forecasting'\n",
    "    \n",
    "        # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "        # Neural network model parameters\n",
    "        #\n",
    "        # Input sequence length - look-back\n",
    "        self.Lag         = 3 * 24\n",
    "        # Prediction sequence length\n",
    "        self.Horizon     = 24\n",
    "        #\n",
    "        self.individual  = False\n",
    "        self.enc_in      = 1\n",
    "        self.kernel_size = 25\n",
    "        \n",
    "        # Training parameters\n",
    "        #\n",
    "        # Number of epochs\n",
    "        self.epochs        = 1000\n",
    "        # Batch size\n",
    "        self.batch_size    = 32\n",
    "        # Number of workers in DataLoader\n",
    "        self.num_workers   = 0\n",
    "        # Define verbose\n",
    "        self.verbose       = True\n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-4\n",
    "        # Trained model path\n",
    "        self.model_path    = 'models/DLinear.pth'\n",
    "        \n",
    "        # Data handling\n",
    "        #\n",
    "        # Filename\n",
    "        self.filename              = './data/Solar_Radiation_Torino.csv'\n",
    "        # Target series name \n",
    "        self.targetSeries          = 'Torre Pellice Direct Shortwave Radiation'\n",
    "        # Training-set percentage\n",
    "        self.TrainingSetPercentage = 0.8\n",
    "        # Data Log-transformation\n",
    "        self.Transformation        = True\n",
    "        # Scaling {'Standard', 'MinMax', 'Robust'}\n",
    "        self.Scaling               = 'Standard'\n",
    "\n",
    "args = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "#\n",
    "start = time.time()\n",
    "\n",
    "# Load data\n",
    "#\n",
    "df = pd.read_csv( args.filename )\n",
    "\n",
    "print('[INFO] Data imported')\n",
    "print('[INFO] Time: %.2f seconds' % (time.time() - start))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to 'datetime64'\n",
    "#\n",
    "df['Date'] = df['Date'].astype('datetime64')\n",
    "\n",
    "# Set index\n",
    "#\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "\n",
    "# Keep only selected time-series\n",
    "#\n",
    "df = pd.DataFrame( df[ [ args.targetSeries ] ] )\n",
    "\n",
    "\n",
    "df.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int( df.shape[0] * args.TrainingSetPercentage )\n",
    "\n",
    "df_train = df[ :idx ].dropna()\n",
    "df_test  = df[ idx: ].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(20, 3) )\n",
    "\n",
    "df_train[ args.targetSeries ].plot(ax=ax, color='tab:blue' )\n",
    "df_test[ args.targetSeries ].plot(ax=ax,  color='tab:orange')\n",
    "\n",
    "plt.legend(['Training', 'Testing'], frameon = False, fontsize = 14)\n",
    "plt.ylabel(args.targetSeries, size = 14)\n",
    "plt.xlabel('Date', size = 14);\n",
    "plt.xticks(size = 12);\n",
    "plt.yticks(size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_train.iloc[-args.Lag:], df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.Transformation == True):\n",
    "    \n",
    "    print('[INFO] Data transformation applied')\n",
    "    \n",
    "    VALUE = np.ceil( max(abs( -df.min().min() ), 1.0) )\n",
    "    \n",
    "    df_train = np.log( df_train + VALUE)\n",
    "    df_test  = np.log( df_test  + VALUE)\n",
    "    \n",
    "else:\n",
    "    print('[INFO] No data transformation applied.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.Scaling == 'MinMax'):\n",
    "    print('[INFO] Scaling: MinMax')\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if (feature ==  args.targetSeries ): continue\n",
    "        print('Feature: ', feature)        \n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = MinMaxScaler()\n",
    "    df_train[ args.targetSeries ] = scaler.fit_transform( df_train[  args.targetSeries  ].to_numpy().reshape(-1,1) )\n",
    "    df_test[ args.targetSeries ]  = scaler.transform( df_test[  args.targetSeries  ].to_numpy().reshape(-1,1) )\n",
    "            \n",
    "elif (args.Scaling == 'Robust'):\n",
    "    print('[INFO] Scaling: Robust')\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if (feature ==  args.targetSeries ): continue\n",
    "        print('Feature: ', feature)        \n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = RobustScaler()\n",
    "    df_train[ args.targetSeries ] = scaler.fit_transform( df_train[  args.targetSeries  ].to_numpy().reshape(-1,1) )\n",
    "    df_test[ args.targetSeries ]  = scaler.transform( df_test[  args.targetSeries  ].to_numpy().reshape(-1,1) )\n",
    "        \n",
    "elif (args.Scaling == 'Standard'):\n",
    "    print('[INFO] Scaling: Standard')\n",
    "\n",
    "    for feature in df.columns:\n",
    "        if (feature ==  args.targetSeries ): continue\n",
    "        print('Feature: ', feature)\n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    df_train[ args.targetSeries ] = scaler.fit_transform( df_train[  args.targetSeries  ].to_numpy().reshape(-1,1) )\n",
    "    df_test[ args.targetSeries ]  = scaler.transform( df_test[  args.targetSeries  ].to_numpy().reshape(-1,1) )             \n",
    "else:\n",
    "    print('[WARNING] Unknown data scaling. Standar scaling was selected')   \n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if (feature ==  args.targetSeries ): continue\n",
    "        print('Feature: ', feature)\n",
    "        # Set scaler\n",
    "        #\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        df_train[feature] = scaler.fit_transform( df_train[ feature ].to_numpy().reshape(-1,1) )\n",
    "        df_test[feature]  = scaler.transform( df_test[ feature ].to_numpy().reshape(-1,1) )\n",
    "\n",
    "        \n",
    "    # Scaling of Target Series\n",
    "    #\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    df_train[ args.targetSeries ] = scaler.fit_transform( df_train[  args.targetSeries  ].to_numpy().reshape(-1,1) )\n",
    "    df_test[ args.targetSeries ]  = scaler.transform( df_test[  args.targetSeries  ].to_numpy().reshape(-1,1) )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class TimeFeature(ABC):\n",
    "    \"\"\"Abstract class for time features\"\"\"\n",
    "    def __init__(self, normalise: bool, a: float, b: float):\n",
    "        self.normalise = normalise\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def _max_val(self) -> float:\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def max_val(self) -> float:\n",
    "        return self._max_val if self.normalise else 1.0\n",
    "\n",
    "    def scale(self, val: np.ndarray) -> np.ndarray:\n",
    "        return val * (self.b - self.a) + self.a\n",
    "\n",
    "    def process(self, val: np.ndarray) -> np.ndarray:\n",
    "        features = self.scale(val / self.max_val)\n",
    "        if self.normalise:\n",
    "            return features\n",
    "        return features.astype(int)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(normalise={self.normalise}, a={self.a}, b={self.b})\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Second of minute, unnormalised: [0, 59]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.second)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 59.0\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour, unnormalised: [0, 59]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.minute)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 59.0\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day, unnormalised: [0, 23]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.hour)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 23.0\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day, unnormalised: [0, 6]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.dayofweek)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 6.0\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month, unnormalised: [0, 30]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.day - 1)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 30.0\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year, unnormalised: [0, 365]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.dayofyear - 1)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 365.0\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year, unnormalised: [0, 52]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(pd.Index(idx.isocalendar().week, dtype=int) - 1)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 52.0\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year, unnormalised: [0, 11]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.month - 1)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 11.0\n",
    "\n",
    "\n",
    "class QuarterOfYear(TimeFeature):\n",
    "    \"\"\"Quarter of year, unnormalised: [0, 3]\"\"\"\n",
    "    def __call__(self, idx: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return self.process(idx.quarter - 1)\n",
    "\n",
    "    @property\n",
    "    def _max_val(self):\n",
    "        return 3.0\n",
    "\n",
    "\n",
    "str_to_feat = {\n",
    "    # dictionary mapping name to TimeFeature function\n",
    "    'SecondOfMinute': SecondOfMinute,\n",
    "    'MinuteOfHour': MinuteOfHour,\n",
    "    'HourOfDay': HourOfDay,\n",
    "    'DayOfWeek': DayOfWeek,\n",
    "    'DayOfMonth': DayOfMonth,\n",
    "    'DayOfYear': DayOfYear,\n",
    "    'WeekOfYear': WeekOfYear,\n",
    "    'MonthOfYear': MonthOfYear,\n",
    "    'QuarterOfYear': QuarterOfYear,\n",
    "}\n",
    "\n",
    "\n",
    "freq_to_feats = {\n",
    "    # dictionary mapping frequency to list of TimeFeature functions\n",
    "    'q': [QuarterOfYear],\n",
    "    'm': [QuarterOfYear, MonthOfYear],\n",
    "    'w': [QuarterOfYear, MonthOfYear, WeekOfYear],\n",
    "    'd': [QuarterOfYear, MonthOfYear, WeekOfYear, DayOfYear, DayOfMonth, DayOfWeek],\n",
    "    'h': [QuarterOfYear, MonthOfYear, WeekOfYear, DayOfYear, DayOfMonth, DayOfWeek, HourOfDay],\n",
    "    't': [QuarterOfYear, MonthOfYear, WeekOfYear, DayOfYear, DayOfMonth, DayOfWeek, HourOfDay, MinuteOfHour],\n",
    "    's': [QuarterOfYear, MonthOfYear, WeekOfYear, DayOfYear, DayOfMonth, DayOfWeek, HourOfDay, MinuteOfHour, SecondOfMinute],\n",
    "}\n",
    "\n",
    "\n",
    "def get_time_features(dates: pd.DatetimeIndex, \n",
    "                      normalise: bool = False, \n",
    "                      a: Optional[float] = 0., \n",
    "                      b: Optional[float] = 1.,\n",
    "                      features: Optional[Union[str, List[str]]] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a numpy array of date/time features based on either frequency or directly specifying a list of features.\n",
    "    :param dates: DatetimeIndex object of shape (time,)\n",
    "    :param normalise: Whether to normalise feature between [a, b]. If not, return as an int in the original feature range.\n",
    "    :param a: Lower bound of feature\n",
    "    :param b: Upper bound of feature\n",
    "    :param features: Frequency string used to obtain list of TimeFeatures, or directly a list of names of TimeFeatures\n",
    "    :return: np array of date/time features of shape (time, n_feats)\n",
    "    \"\"\"\n",
    "    if isinstance(features, list):\n",
    "        assert all([feat in str_to_feat.keys() for feat in features]), \\\n",
    "            f\"items in list should be one of {[*str_to_feat.keys()]}\"\n",
    "        features = [str_to_feat[feat] for feat in features]\n",
    "    elif isinstance(features, str):\n",
    "        assert features in freq_to_feats.keys(), \\\n",
    "            f\"features should be one of {[*freq_to_feats.keys()]}\"\n",
    "        features = freq_to_feats[features]\n",
    "    else:\n",
    "        print('[ERROR]')\n",
    "        raise ValueError(f\"features should be a list or str, not a {type(features)}\")\n",
    "\n",
    "    features = [feat(normalise, a, b)(dates) for feat in features]\n",
    "\n",
    "    if len(features) == 0:\n",
    "        return np.empty((dates.shape[0], 0))\n",
    "    return np.stack(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_to_feat = {\n",
    "#     # dictionary mapping name to TimeFeature function\n",
    "#     'SecondOfMinute': SecondOfMinute,\n",
    "#     'MinuteOfHour': MinuteOfHour,\n",
    "#     'HourOfDay': HourOfDay,\n",
    "#     'DayOfWeek': DayOfWeek,\n",
    "#     'DayOfMonth': DayOfMonth,\n",
    "#     'DayOfYear': DayOfYear,\n",
    "#     'WeekOfYear': WeekOfYear,\n",
    "#     'MonthOfYear': MonthOfYear,\n",
    "#     'QuarterOfYear': QuarterOfYear,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training/Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df = None, Lag = 1, Horizon = 1, targetSeries = None, overlap = 1):\n",
    "    \n",
    "    if (targetSeries is None):\n",
    "        targetSeries = df.columns[-1]\n",
    "    \n",
    "    dataX, dataY, dataDate, dataXTime, dataYTime = [], [], [], [], []\n",
    "    \n",
    "    for i in tqdm( range(0, df.shape[0] + 1  - Lag - Horizon, overlap) ):\n",
    "        \n",
    "        dataX.append( df.to_numpy()[i:(i+Lag)] )        \n",
    "        dataY.append( df[ targetSeries ].to_numpy()[i + Lag : i + Lag + Horizon] )\n",
    "        dataDate.append( df.index[i + Lag : i + Lag + Horizon].tolist() )\n",
    "        #\n",
    "        dataXTime.append( get_time_features(dates    = df.index[i : i+Lag],  \n",
    "                                            features = ['DayOfWeek', 'HourOfDay']))\n",
    "        dataYTime.append( get_time_features(dates    = df.index[i + Lag : i + Lag + Horizon],  \n",
    "                                            features = ['DayOfWeek', 'HourOfDay']))\n",
    "\n",
    "\n",
    "    return ( np.array(dataX).astype(np.float32), \n",
    "             np.array(dataY).astype(np.float32), \n",
    "             np.array(dataDate),\n",
    "             np.array(dataXTime).astype(np.float32), \n",
    "             np.array(dataYTime).astype(np.float32) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, _, trainXTime, trainYTime      = create_dataset(df           = df_train, \n",
    "                                                                Lag          = args.Lag, \n",
    "                                                                Horizon      = args.Horizon, \n",
    "                                                                targetSeries = args.targetSeries,\n",
    "                                                                overlap      = 1,)\n",
    "                               \n",
    "\n",
    "testX,  testY, testDate, testXTime, testYTime  = create_dataset(df           = df_test, \n",
    "                                                                Lag          = args.Lag, \n",
    "                                                                Horizon      = args.Horizon, \n",
    "                                                                targetSeries = args.targetSeries,\n",
    "                                                                overlap      = 1,)\n",
    "\n",
    "\n",
    "# Last 10% of the training data will be used for validation\n",
    "#\n",
    "idx = int(0.9 * trainX.shape[0])\n",
    "validX, validY, validXTime, validYTime = trainX[ idx: ], trainY[ idx: ], trainXTime[ idx: ], trainYTime[ idx: ]\n",
    "trainX, trainY, trainXTime, trainYTime = trainX[ :idx ], trainY[ :idx ], trainXTime[ :idx ], trainYTime[ :idx ]\n",
    "\n",
    "print('Training data shape:   ', trainX.shape, trainY.shape)\n",
    "print('Validation data shape: ', validX.shape, validY.shape)\n",
    "print('Testing data shape:    ', testX.shape,  testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshaping\n",
    "# #\n",
    "# trainY = np.expand_dims(trainY, axis = -1)\n",
    "# validY = np.expand_dims(validY, axis = -1)\n",
    "# testY  = np.expand_dims(testY,  axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data( Dataset ):\n",
    "    def __init__(self, X, Y, XTime, YTime):\n",
    "        self.X    = X\n",
    "        self.Y    = Y\n",
    "        self.XTime = XTime\n",
    "        self.YTime = YTime\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[ idx ], self.Y[ idx ], self.XTime[ idx ], self.YTime[ idx ]\n",
    "    \n",
    "\n",
    "    \n",
    "# Create training and test dataloaders\n",
    "#\n",
    "train_ds = Data(trainX, trainY, trainXTime, trainYTime)\n",
    "valid_ds = Data(validX, validY, validXTime, validYTime)\n",
    "test_ds  = Data(testX,  testY,  testXTime,  testYTime)\n",
    "\n",
    "\n",
    "# Prepare Data-Loaders\n",
    "#\n",
    "train_dl = DataLoader(train_ds, batch_size = args.batch_size, num_workers = args.num_workers)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = args.batch_size, num_workers = args.num_workers)\n",
    "test_dl  = DataLoader(test_ds,  batch_size = args.batch_size, num_workers = args.num_workers)\n",
    "#\n",
    "print('[INFO] Data loaders were created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting model: DeepTIMe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neural Network\n",
    "# \n",
    "model = DeepTIMe(datetime_feats  = trainXTime.shape[-1], \n",
    "                 layer_size      = 64, \n",
    "                 inr_layers      = 32, \n",
    "                 n_fourier_feats = 10, \n",
    "                 scales          = [0.1])\n",
    "\n",
    "\n",
    "model.to( device )\n",
    "\n",
    "\n",
    "print( model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "#\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Specify loss function\n",
    "#\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), \n",
    "                             lr     = args.learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "#\n",
    "early_stopping = EarlyStopping(patience  = 30,\n",
    "                               min_delta = 1e-5)\n",
    "\n",
    "\n",
    "# LR scheduler\n",
    "#\n",
    "scheduler = LRScheduler(optimizer = optimizer, \n",
    "                        patience  = 10, \n",
    "                        min_lr    = 1e-10, \n",
    "                        factor    = 0.5, \n",
    "                        verbose   = args.verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store training and validation loss\n",
    "Loss = {\n",
    "         'Train': [], \n",
    "         'Valid':  []\n",
    "       }\n",
    "\n",
    "\n",
    "\n",
    "# Set number at how many iteration the training process (results) will be provided\n",
    "#\n",
    "batch_show = (train_dl.dataset.__len__() // args. batch_size // 5)\n",
    "\n",
    "\n",
    "\n",
    "# Main loop - Training process\n",
    "#\n",
    "for epoch in range(1, args.epochs+1):\n",
    "\n",
    "    # Start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # Monitor training loss\n",
    "    #\n",
    "    train_loss = 0.0\n",
    "    valid_loss  = 0.0    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################\n",
    "    # Train the model #\n",
    "    ###################\n",
    "    batch_idx = 0\n",
    "    for data, target, XTime, YTime in train_dl:\n",
    "        \n",
    "        # Clear the gradients of all optimized variables\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        #\n",
    "        if (device.type == 'cpu'):\n",
    "            data   = torch.tensor(data,   dtype=torch.float32)\n",
    "            target = torch.tensor(target, dtype=torch.float32)\n",
    "        else:\n",
    "            data   = torch.tensor(data,   dtype=torch.float32).cuda()\n",
    "            target = torch.tensor(target, dtype=torch.float32).cuda()\n",
    "\n",
    "            \n",
    "        outputs = model( data, XTime, YTime ).squeeze(-1)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Calculate the loss\n",
    "        #\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        #\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Perform a single optimization step (parameter update)\n",
    "        #\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Update running training loss\n",
    "        #\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "               \n",
    "        # Increase batch_idx\n",
    "        #\n",
    "        batch_idx  += 1\n",
    "        \n",
    "        \n",
    "        # Info\n",
    "        #\n",
    "        if (args.verbose == True and batch_idx % batch_show == 0):\n",
    "            print('> Epoch: {} [{:5.0f}/{} ({:.0f}%)]'.format(epoch, batch_idx * len(data), len(train_dl.dataset), 100. * batch_idx / len(train_dl)))        \n",
    "\n",
    "           \n",
    "        \n",
    "    # Print avg training statistics \n",
    "    #\n",
    "    train_loss = train_loss / train_dl.dataset.X.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target, XTime, YTime in valid_dl:\n",
    "\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            #\n",
    "            if (device.type == 'cpu'):\n",
    "                data   = torch.tensor(data, dtype=torch.float32)\n",
    "                target = torch.tensor(target, dtype=torch.float32)\n",
    "            else:\n",
    "                data   = torch.tensor(data, dtype=torch.float32).cuda()\n",
    "                target = torch.tensor(target, dtype=torch.float32).cuda()\n",
    "\n",
    "\n",
    "            outputs = model( data, XTime, YTime ).squeeze(-1)\n",
    "        \n",
    "          \n",
    "\n",
    "            # Calculate the loss\n",
    "            #\n",
    "            loss = criterion(outputs, target)\n",
    "                \n",
    "            # update running training loss\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "              \n",
    "\n",
    "    # Print avg training statistics \n",
    "    #\n",
    "    valid_loss = valid_loss / test_dl.dataset.X.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Stop timer\n",
    "    #\n",
    "    stop  = time.time()\n",
    "    \n",
    "    \n",
    "    # Show training results\n",
    "    #\n",
    "    print('\\n[INFO] Train Loss: {:.6f}\\tValid Loss: {:.6f} \\tTime: {:.2f}secs'.format(train_loss, valid_loss, stop-start), end=' ')\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    # Update best model\n",
    "    #\n",
    "    if (epoch == 1):\n",
    "        Best_score = valid_loss\n",
    "        \n",
    "        torch.save(model.state_dict(), args.model_path)\n",
    "        print('(Model saved)\\n')\n",
    "    else:\n",
    "        if (Best_score > valid_loss):\n",
    "            Best_score = valid_loss\n",
    "            \n",
    "            torch.save(model.state_dict(), args.model_path)\n",
    "            print('(Model saved)\\n')\n",
    "        else:\n",
    "            print('\\n')\n",
    "     \n",
    "    \n",
    "    # Store train/val loss\n",
    "    #\n",
    "    Loss['Train'] += [ train_loss ]\n",
    "    Loss['Valid'] += [ valid_loss ]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "    # Learning rate scheduler\n",
    "    #\n",
    "    scheduler( valid_loss )\n",
    "    \n",
    "    \n",
    "    # =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "    # Early Stopping\n",
    "    #\n",
    "    if ( early_stopping( valid_loss ) ): break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "#\n",
    "model.load_state_dict( torch.load( args.model_path ) );\n",
    "model.eval();\n",
    "\n",
    "print('[INFO] Model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = None\n",
    "with torch.no_grad():\n",
    "    for data, target, XTime, YTime in tqdm( test_dl ):\n",
    "\n",
    "        data   = torch.tensor(data,   dtype=torch.float32)\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "        if (pred is None):\n",
    "            pred = model( data, XTime, YTime ).squeeze(-1).detach().numpy()\n",
    "        else:\n",
    "            pred = np.concatenate([ pred, model( data, XTime, YTime ).squeeze(-1).detach().numpy() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshaping...\n",
    "# #\n",
    "# testY = testY.squeeze(-1)\n",
    "# pred  = pred.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply inverse scaling/transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply inverse scaling\n",
    "#\n",
    "for i in range( args.Horizon ):\n",
    "    testY[:,  i] = scaler.inverse_transform( testY[:,  i].reshape(-1,1) ).squeeze(-1)\n",
    "    pred[:, i]   = scaler.inverse_transform( pred[:, i].reshape(-1,1) ).squeeze(-1)\n",
    "\n",
    "\n",
    "# Apply inverse transformation   \n",
    "#\n",
    "if (args.Transformation == True):\n",
    "    testY = np.exp( testY ) - VALUE\n",
    "    pred  = np.exp( pred )  - VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Performance on Testing set - Prediction visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('[INFO] Feature: ', args.targetSeries)\n",
    "print('------------------------------------------------')\n",
    "Performance_Foresting_Model = {'RMSE': [], 'MAE': [], 'SMAPE': [], 'R2' : []}\n",
    "\n",
    "for i in range( args.Horizon ):\n",
    "\n",
    "    Prices = pd.DataFrame([])        \n",
    "\n",
    "    Prices[ args.targetSeries ] = testY[:,i]\n",
    "    Prices[ 'Prediction'      ] = pred[:,i]\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    #\n",
    "    MAE, RMSE, MAPE, SMAPE, R2 = RegressionEvaluation( Prices )\n",
    "\n",
    "    # Store results\n",
    "    #\n",
    "    Performance_Foresting_Model['RMSE']    += [ RMSE    ]\n",
    "    Performance_Foresting_Model['MAE']     += [ MAE     ]\n",
    "    Performance_Foresting_Model['SMAPE']   += [ SMAPE   ]\n",
    "    Performance_Foresting_Model['R2']      += [ R2      ]\n",
    "\n",
    "    # Present results\n",
    "    #\n",
    "    print('Horizon: %2i MAE %5.2f RMSE %5.2f SMAPE: %5.2f R2: %.2f' % (i+1, MAE, RMSE, SMAPE, R2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# for i in range( args.Horizon ):\n",
    "\n",
    "#     # Get actual values and predicted\n",
    "#     #\n",
    "#     Prices = pd.DataFrame([])        \n",
    "\n",
    "#     Prices[ args.targetSeries ] = testY[:,i]\n",
    "#     Prices[ 'Prediction'      ] = pred[:,i]\n",
    "                        \n",
    "#     # Calculate the residuals\n",
    "#     #\n",
    "#     res = (Prices[ args.targetSeries ] - Prices['Prediction']).to_numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # === Visualization ===\n",
    "#     #\n",
    "#     fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 2) )\n",
    "\n",
    "#     # Plot residual histogram\n",
    "#     #\n",
    "#     ax[0].hist(res, bins = 50)    \n",
    "    \n",
    "#     # Plot AutoCorrelation plot\n",
    "#     #\n",
    "#     plot_acf( res, ax=ax[1] )       \n",
    "#     ax[1].set_ylim([-1.05, 1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply inverse scaling on trainX\n",
    "# #\n",
    "# for i in range( trainX.shape[1] ):\n",
    "#     testX[:,  i, 0] = scaler.inverse_transform( testX[:, i, 0].reshape(-1,1) ).squeeze(-1)\n",
    "\n",
    "\n",
    "# # Apply inverse transformation   \n",
    "# #\n",
    "# if (Transformation == True):\n",
    "#     testX = np.exp( testX ) - VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplots = [331, 332, 333, 334, 335, 336,  337, 338, 339]\n",
    "plt.figure( figsize = (20, 15) )\n",
    "\n",
    "# Select random cases\n",
    "RandomInstances = [random.randint(1, testY.shape[0]) for i in range(0, 9)]\n",
    "\n",
    "\n",
    "for plot_id, i in enumerate(RandomInstances):\n",
    "\n",
    "    plt.subplot(subplots[plot_id])\n",
    "    plt.grid()\n",
    "#     plot_scatter(range(0, Lag), testX[i,:,0], color='b')\n",
    "    plt.plot(testDate[i], testY[i], color='g', marker = 'o', linewidth = 2)\n",
    "    plt.plot(testDate[i], pred[i],  color='r', marker = 'o', linewidth = 2)\n",
    "\n",
    "    plt.legend(['Actual values', 'Prediction'], frameon = False, fontsize = 12)\n",
    "#     plt.ylim([0, 100])\n",
    "    plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3f87851f675c6afefcd51f967c1836fa00f505afe5b318b834b199856e321654"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
